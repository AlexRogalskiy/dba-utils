# Общее описание

Архив представляет собой неотъемлемую часть *отказоустойчивой* инфраструктуры, связанной с Базами Данных PostgreSQL.

Эта часть отвечает за:

- отправку, проигрывание, хранение, ротацию WAL.
- очередь бэкапов, бэкапы, ротацию бэкапов (проверка бэкапов не входит в Archive 2.0 и представляет собой отдельную инфраструктуру).
- PITR, восстановление из бэкапов (косвенно).
- репликацию (в случае Avito, в большинстве кластеров БД, кроме тех, где есть синхронная streaming-репликация).

Особенность Archive 2.0 в сравнении с другими системами заключается в том, что WAL хранятся сразу на двух архив-серверах (компоненты системы будут рассмотрены ниже).

При исчезновении доступа к одному из архив-серверов, система продолжает работать. "Дырки" в потоке WAL синхронизируются при каждом бэкапе.

Бэкапы выполняются по очереди на двух архиверах при помощи *pg_basebackup*. Всегда есть окно восстановления состояния БД из бэкапа на указанное в настройках кластера число дней до текущего момента (или момента падения мастера).

При использовании archive-based репликации, гарантируется утверждение "архив всегда впереди standby", что исключает потерю данных, необходимых для PITR ("дырок" в потоке WAL).
# Компоненты системы

- **два архив-сервера** - на них хранятся WAL'ы и бэкапы.
- **archive_cmd2** - архив-команда для PostgreSQL конфига на мастер-сервере, отправляет WAL и другие файлы потока на один из двух доступных архив-серверов.
- **archive_remote_cmd_2** - программа со стороны архив-серверов, которая принимает WAL и отправляет на второй "архивер" (он же архив-сервер).
- **restore_cmd_2** - программа, отвечающая за проигрывание WAL на standby (указывается в recovery.conf).
- **backup_queue** - база данных, в которой хранится саморегулируемое расписание бэкапов, настройки бэкапов и статусы завершения бэкапов.
- **base-backup_2** - программа, запускаемая на архиверах по крону раз в 10 минут для "взятия" задачи бэкапа из очереди и выполнения самого бэкапа. Состоит из:
- **base-backup_2** - сам скрипт
- **wal-cleanup_2** - чистка ненужных WAL (не входящих ни в один хранящийся в данный момент времени бэкап).
- **wal-sync** - синхронизация в обе стороны двух директорий с WAL, находящихся на двух разных серверах.
- **restore_cmd_2**  - проигрывает WAL на standby с одного из двух доступных серверов-архиверов.
- **мониторинг** - мониторится, как минимум, очередь бэкапов и срабатывают алерты при поломке одного из бэкапов.

# Описание алгоритма "пути WAL" до архива

Мастер-сервер выполняет из конфига archive_command вида:

```sh
archive_command = '/usr/local/bin/archive_cmd_2 \'hostname_archive_1 hostname_archive_2\' /storage/archive_directory/ %p %f cluster_name' 
```

Описание аргументов:
```sh
 DST-HOSTNAMES              - two archive host names in single quotes
 DST-DIR                    - archive directory for WALs (ssh path)
 SRC-WAL-FILENAME-WITH-PATH - %p (file name with path)
 SRC-WAL-FILENAME           - %f (file name)
 CLUSTER-NAME               - unique cluster name
```
Стоит обратить внимание на DST-DIR - это путь на **АРХИВ**-сервере.


*archive_cmd_2* отправляет wal'ы и сопутствующие файлы на два архив-хоста по схеме:
```sh
[ master: archive_cmd ] -> [ archive1: archive_remote_cmd ] -> [ archive2: scp ]
```
*archive_cmd_2* пишет на первый из двух DST-HOSTNAME через archive_remote_cmd_2 на удаленном хосте.

*archive_remote_cmd* (принимающая сторона на одном из архиверов) пишет WAL на себя и пытается отправить на SYNC-HOST.

Если *archive_cmd_2* не удается записать WAL или другой файл на первый хост из аргумента, то через N попыток, команда начинать писать WAL'ы только на второй хост в течение N секунд (cooldown_time):
```sh
retry_count="6"     # see below
cooldown_time="600" # do not try to send file to $dst_host1 for a '$cooldown_time' seconds after '$retry_count' attemps
```
